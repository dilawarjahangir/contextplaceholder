

I gave a prompt to LLM modeal and it has generated me four responses.

I am going to provide you the prompt and the responses which were created by my LLM model and your job is to evaluate the response based on the criteria that i will provide you.


This is my prompt:

{{prompt}}



Response Evaluation
Evaluation Form - PENDING
Making Sense *
The LLM provides responses that fail to meaningfully engage with the user’s input. This includes:

Repeating the same information unnecessarily.
Restating the prompt without providing additional insight.
Refusing to answer valid queries without proper justification
These behaviors disrupt the conversation flow, reduce user satisfaction, and hinder productive interaction.



Major Issues - Mistakes that negatively affect the user experience in significant/critical ways (The user gets little or no value, perhaps negative value).

Moderate Issues - Mistakes that partially affect the user experience. (The user gets some value, but significant improvement could be made).

Minor Issues - Mistakes that may not affect the user experience or affect it in trivial ways. (The user still gets most of the value, perhaps there’s room for improvement).

No Issues - No mistakes are made. (User gets full & optimal value)

Instruction Following *
Is the provided response on-point & respects all constraints in the user prompt? Is it tailored for the User skill level?

Core areas we should look for in this case:

Comprehends and adheres to all constraints and requests of user
Addresses all the requests of the user (Exceptions will be requests that are outside the capability of the LLM. For example: Give me a sorting algorithm with O(log(n)) time OR Give me the production ready React app to track student data.)
Focus remains on the user’s request
Not too short to skip the important and helpful information
Not too verbose to include unnecessary details
Well-tailored for the skill level of the user


Major Issues - Mistakes that negatively affect the user experience in significant/critical ways (The user gets little or no value, perhaps negative value).

Moderate Issues - Mistakes that partially affect the user experience. (The user gets some value, but significant improvement could be made).

Minor Issues - Mistakes that may not affect the user experience or affect it in trivial ways. (The user still gets most of the value, perhaps there’s room for improvement).

No Issues - No mistakes are made. (User gets full & optimal value)

Accuracy *
Does the AI's response correctly and completely address the information and code requirements?

Core areas we should look for in this case:

Factual correctness
Comprehensive Answer (No missing key points)
Code syntax errors
Code functional errors


Major Issues - Mistakes that negatively affect the user experience in significant/critical ways (The user gets little or no value, perhaps negative value).

Moderate Issues - Mistakes that partially affect the user experience. (The user gets some value, but significant improvement could be made).

Minor Issues - Mistakes that may not affect the user experience or affect it in trivial ways. (The user still gets most of the value, perhaps there’s room for improvement).

No Issues - No mistakes are made. (User gets full & optimal value)

Efficiency *
Is the AI's response optimal in terms of the approach, code complexity, case coverage, and the method suggested in response to the user's prompt?

Core areas we should look for in this case:

Optimality in terms of Time and Memory complexity (It is fine if assistant gives an algorithm which is efficient and used in mainstream rather than a complex algorithm which optimizes on time/memory a little bit more.)
Handles all the edge cases
Takes care of security aspect of code
During Q&A, suggest optimal answer to the user


Major Issues - Mistakes that negatively affect the user experience in significant/critical ways (The user gets little or no value, perhaps negative value).

Moderate Issues - Mistakes that partially affect the user experience. (The user gets some value, but significant improvement could be made).

Minor Issues - Mistakes that may not affect the user experience or affect it in trivial ways. (The user still gets most of the value, perhaps there’s room for improvement).

No Issues - No mistakes are made. (User gets full & optimal value)

Presentation *
Is the presentation of the AI's response clear and well-organized?



General presentation rules for code output:

Docstrings are not needed but complex code lines should include comments detailing logic and behavior
Test outputs include a comment with the expected response
Explanations are presented in a clear manner using bullet points.
Key terms are highlighted in bold whereas titles, articles, etc are italicized
Response doesn’t give multiple redundant code solutions to solve the same problem
Multi-line code blocks are wrapped in triple backticks with correct language specified after upper backticks to ensure proper indentation and formatting
Markdown syntax is correct and represents a proper hierarchy
White space and line breaks are used to improve readability and separate content sections
Tables are constructed with Hyphens and Pipes and are correctly lined up
Comments are clear and easily understood


﻿Major Issues - Mistakes that negatively affect the user experience in significant/critical ways (The user gets little or no value, perhaps negative value).

Moderate Issues - Mistakes that partially affect the user experience. (The user gets some value, but significant improvement could be made).

Minor Issues - Mistakes that may not affect the user experience or affect it in trivial ways. (The user still gets most of the value, perhaps there’s room for improvement).

No Issues - No mistakes are made. (User gets full & optimal value)

Up-to-date *
Does the code use any deprecated libraries to solve the problem?

Core areas we should look for in this case:

Stick to the latest stable versions of languages and libraries to avoid deprecated functions and gain performance benefits.
Apply modern language features for better readability and efficiency, such as Python’s f-strings.
Regularly update dependencies to incorporate the latest security patches using a dependency management tool.
Follow current best practices and coding standards to ensure security and efficiency.
Utilize optimized algorithms and data structures, leveraging built-in functions where possible.
Write clear, maintainable code with appropriate comments, especially for complex areas.
Ensure compatibility and functionality with unit tests and automated CI/CD pipelines.
Stay informed on updates and changes within relevant communities or ecosystems.
Where necessary, maintain backwards compatibility while providing clear upgrade paths.




Major Issues: Significant mistakes that critically endanger code functionality or security (e.g., not implementing security patches).

Moderate Issues: Partially affect code functionality or long-term maintainability (e.g., use of outdated syntax).

Minor Issues: Trivial impact on code quality or could be fine-tuned for best practices (e.g., missing minor optimizations).

No Issues: Code follows all guidelines effectively, providing full and optimal value.

Executable code *
Please add an additional annotation regarding the following 4 scenarios for model code block response types when the assistant outputs a large code block:

No Issues: The model provides a complete code block that should run out of the box (even if there are bugs or logical issues).
Minor Issues: The model updates a single function that’s meant to replace or integrate into the user’s broader code context by copy-pasting.
Moderate Issues: The model provides a partial code block, such as the first half or second half of the code, with a comment indicating where the remaining code should go.
Provides comment at the end or the beginning of the mode code response:
// rest of your code here
Major Issues: The model responds with only a skeleton of the whole code block, with empty functions and comments in each function. Example:
def some_func(args: Any) -> Any: 
// your code here
Code Output after Execution *
Choose the option that best describes the code output in the model's response:



Yes: The code output included by the model in the response matches the output obtained after executing the code.

No: The code output included by the model in the response does not match the output obtained after executing the code.

N/A: No code output in the response from the model.

Other Issues *
Are there any other significant issues in the response that affect user experience but were not covered in the predefined categories?

We want you to not be confined by predefined categories while accessing the quality of response. For example: 1) Assistant forgets the context of its previous conversation. 2) The React Component provided by the assistant has very bad user experience. 3) Become overly apologetic about the things that can’t be done by the assistant.


Major Issues - Mistakes that negatively affect the user experience in significant/critical ways (The user gets little or no value, perhaps negative value).

Moderate Issues - Mistakes that partially affect the user experience. (The user gets some value, but significant improvement could be made).

Minor Issues - Mistakes that may not affect the user experience or affect it in trivial ways. (The user still gets most of the value, perhaps there’s room for improvement).

No Issues - No mistakes are made. (User gets full & optimal value)

Rating Justification *
We will use this field to put:

Reasoning for the marked Major/Minor Issues.
Details of the ‘Other Issues’ if it is marked Minor/Major.
Comments for Reference *
We will use this field to put:

Any notes that you want to convey to the reviewer. You can also include links to screenshots or a sandbox where you have tested something related to the response.
Final Score *
These scores are relative to each other. So, rate the scores as a preference for each model as a rating.

5- Exemplary

4- Good

3- Fair

2- Bad

1- Terrible


N/A


This is evaluation Criteria and i will provide you the responses now.